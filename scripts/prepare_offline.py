#!/usr/bin/env python3
"""
ç¦»çº¿è¯„æµ‹èµ„æºå‡†å¤‡è„šæœ¬

åŠŸèƒ½ï¼š
1. æ‰«æ /data ç›®å½•è·å–æ‰€æœ‰æ•°æ®é›†åˆ—è¡¨
2. ç¡®ä¿è¿™äº›æ•°æ®é›†å·²ä¸‹è½½åˆ° HuggingFace ç¼“å­˜
3. ä¸‹è½½å¸¸ç”¨è¯„æµ‹æŒ‡æ ‡
4. è®¾ç½®ç¦»çº¿æ¨¡å¼ç¯å¢ƒå˜é‡

ä½¿ç”¨æ–¹æ³•ï¼š
    python scripts/prepare_offline.py

é€‰é¡¹ï¼š
    --download-metrics   ä¸‹è½½è¯„æµ‹æŒ‡æ ‡åˆ°æœ¬åœ°
    --verify-only        ä»…éªŒè¯æ•°æ®é›†æ˜¯å¦å®Œæ•´ï¼Œä¸ä¸‹è½½
    --force              å¼ºåˆ¶é‡æ–°ä¸‹è½½æ‰€æœ‰èµ„æº
"""

import os
import sys
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
import json
import argparse
import shutil

# é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = Path(__file__).parent.parent
DATA_DIR = PROJECT_ROOT / "assets" / "data"
METRICS_CACHE_DIR = PROJECT_ROOT / "outputs" / "cache" / "metrics"

# å¸¸ç”¨è¯„æµ‹æŒ‡æ ‡åˆ—è¡¨ (HuggingFace evaluate åº“)
COMMON_METRICS = [
    "exact_match",
    "accuracy",
    "f1",
    "precision",
    "recall",
    "bleu",
    "rouge",
    "sacrebleu",
    "bertscore",
    "perplexity",
]


def get_local_datasets() -> List[Dict[str, Any]]:
    """æ‰«æ /data ç›®å½•è·å–æ‰€æœ‰æœ¬åœ°æ•°æ®é›†ä¿¡æ¯"""
    datasets = []
    
    if not DATA_DIR.exists():
        print(f"[è­¦å‘Š] æ•°æ®ç›®å½•ä¸å­˜åœ¨: {DATA_DIR}")
        return []
    
    # éå†é¡¶å±‚ç›®å½•
    for dir_path in DATA_DIR.iterdir():
        if not dir_path.is_dir():
            continue
        if dir_path.name.startswith('.') or dir_path.name in ['datasets_metadata', 'tasks', 'tokenizers']:
            continue
        
        dataset_name = dir_path.name
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯ç›´æ¥æ•°æ®é›†
        is_direct_dataset = (
            (dir_path / 'dataset_dict.json').exists() or 
            (dir_path / 'dataset_info.json').exists()
        )
        
        if is_direct_dataset:
            datasets.append({
                "name": dataset_name,
                "path": str(dir_path),
                "is_group": False,
                "subtasks": []
            })
        else:
            # æ£€æŸ¥å­ç›®å½•
            subtasks = []
            for sub_dir in dir_path.iterdir():
                if sub_dir.is_dir():
                    if ((sub_dir / 'dataset_dict.json').exists() or 
                        (sub_dir / 'dataset_info.json').exists()):
                        subtasks.append({
                            "name": sub_dir.name,
                            "path": str(sub_dir)
                        })
            
            if subtasks:
                datasets.append({
                    "name": dataset_name,
                    "path": str(dir_path),
                    "is_group": True,
                    "subtasks": subtasks
                })
    
    return datasets


def verify_dataset(dataset_path: str) -> Tuple[bool, str]:
    """éªŒè¯æ•°æ®é›†æ˜¯å¦å®Œæ•´å¯ç”¨"""
    try:
        from datasets import load_from_disk
        
        path = Path(dataset_path)
        if not path.exists():
            return False, f"è·¯å¾„ä¸å­˜åœ¨: {dataset_path}"
        
        # å°è¯•åŠ è½½æ•°æ®é›†
        dataset = load_from_disk(dataset_path)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ splits
        if not dataset:
            return False, "æ•°æ®é›†ä¸ºç©º"
        
        splits = list(dataset.keys())
        if not splits:
            return False, "æ²¡æœ‰æ‰¾åˆ°ä»»ä½• splits"
        
        # æ£€æŸ¥æ¯ä¸ª split æ˜¯å¦æœ‰æ•°æ®
        for split_name in splits:
            split_data = dataset[split_name]
            if len(split_data) == 0:
                return False, f"Split '{split_name}' ä¸ºç©º"
        
        return True, f"æœ‰æ•ˆ (splits: {', '.join(splits)})"
        
    except Exception as e:
        return False, f"åŠ è½½å¤±è´¥: {str(e)}"


def download_metrics(metrics: List[str] = None, force: bool = False):
    """ä¸‹è½½è¯„æµ‹æŒ‡æ ‡åˆ°æœ¬åœ°ç¼“å­˜"""
    if metrics is None:
        metrics = COMMON_METRICS
    
    try:
        import evaluate
    except ImportError:
        print("[è­¦å‘Š] evaluate åº“æœªå®‰è£…ï¼Œè·³è¿‡æŒ‡æ ‡ä¸‹è½½")
        print("  æç¤º: è¿è¡Œ pip install evaluate å®‰è£…")
        return
    
    METRICS_CACHE_DIR.mkdir(parents=True, exist_ok=True)
    
    print(f"\n{'='*60}")
    print(f"ä¸‹è½½è¯„æµ‹æŒ‡æ ‡åˆ°æœ¬åœ°ç¼“å­˜")
    print(f"ç¼“å­˜ç›®å½•: {METRICS_CACHE_DIR}")
    print(f"{'='*60}")
    
    success_count = 0
    fail_count = 0
    
    for metric_name in metrics:
        print(f"\næ­£åœ¨å¤„ç†: {metric_name}")
        try:
            # å°è¯•åŠ è½½æŒ‡æ ‡ï¼ˆè¿™ä¼šè§¦å‘ä¸‹è½½ï¼‰
            metric = evaluate.load(metric_name)
            print(f"  âœ“ æŒ‡æ ‡ '{metric_name}' å·²ç¼“å­˜")
            success_count += 1
        except Exception as e:
            print(f"  âœ— ä¸‹è½½å¤±è´¥: {e}")
            fail_count += 1
    
    print(f"\næŒ‡æ ‡ä¸‹è½½å®Œæˆ: æˆåŠŸ {success_count} ä¸ª, å¤±è´¥ {fail_count} ä¸ª")


def create_offline_config():
    """åˆ›å»ºç¦»çº¿æ¨¡å¼é…ç½®æ–‡ä»¶"""
    config_path = PROJECT_ROOT / "configs" / "offline_config.json"
    cache_dir = PROJECT_ROOT / "outputs" / "cache"
    hf_cache_dir = cache_dir / "huggingface"
    
    config = {
        "offline_mode": True,
        "data_dir": str(DATA_DIR),
        "cache_dir": str(cache_dir),
        "hf_cache_dir": str(hf_cache_dir),
        "environment_variables": {
            "HF_HOME": str(hf_cache_dir),
            "HF_DATASETS_CACHE": str(hf_cache_dir / "datasets"),
            "TRANSFORMERS_CACHE": str(hf_cache_dir / "transformers"),
            "HF_EVALUATE_CACHE": str(hf_cache_dir / "evaluate"),
            "HF_DATASETS_OFFLINE": "1",
            "HF_HUB_OFFLINE": "1",
            "TRANSFORMERS_OFFLINE": "1"
        }
    }
    
    with open(config_path, "w", encoding="utf-8") as f:
        json.dump(config, f, ensure_ascii=False, indent=2)
    
    print(f"\nç¦»çº¿é…ç½®å·²ä¿å­˜åˆ°: {config_path}")
    return config_path


def print_offline_instructions():
    """æ‰“å°ç¦»çº¿ä½¿ç”¨è¯´æ˜"""
    print(f"""
{'='*60}
ç¦»çº¿è¯„æµ‹ä½¿ç”¨è¯´æ˜
{'='*60}

1. è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆåœ¨è¿è¡Œè¯„æµ‹å‰æ‰§è¡Œï¼‰:

   Linux/Mac:
   export HF_DATASETS_OFFLINE=1
   export HF_HUB_OFFLINE=1
   export TRANSFORMERS_OFFLINE=1

   Windows PowerShell:
   $env:HF_DATASETS_OFFLINE="1"
   $env:HF_HUB_OFFLINE="1"
   $env:TRANSFORMERS_OFFLINE="1"

   Windows CMD:
   set HF_DATASETS_OFFLINE=1
   set HF_HUB_OFFLINE=1
   set TRANSFORMERS_OFFLINE=1

2. å¯åŠ¨åç«¯æœåŠ¡:
   python web_backend/app.py

3. æ•°æ®é›†ä½ç½®: {DATA_DIR}

æ³¨æ„: å·²ä¿®æ”¹çš„ lm_eval ä»£ç ä¼šè‡ªåŠ¨æ£€æµ‹ç¦»çº¿æ¨¡å¼å¹¶ä½¿ç”¨æœ¬åœ°æ•°æ®ã€‚
{'='*60}
""")


def main():
    parser = argparse.ArgumentParser(description="ç¦»çº¿è¯„æµ‹èµ„æºå‡†å¤‡è„šæœ¬")
    parser.add_argument(
        "--download-metrics",
        action="store_true",
        help="ä¸‹è½½è¯„æµ‹æŒ‡æ ‡åˆ°æœ¬åœ°"
    )
    parser.add_argument(
        "--verify-only",
        action="store_true",
        help="ä»…éªŒè¯æ•°æ®é›†æ˜¯å¦å®Œæ•´ï¼Œä¸ä¸‹è½½"
    )
    parser.add_argument(
        "--force",
        action="store_true",
        help="å¼ºåˆ¶é‡æ–°ä¸‹è½½æ‰€æœ‰èµ„æº"
    )
    args = parser.parse_args()
    
    # è®¾ç½®ç¼“å­˜ç›®å½•åˆ°é¡¹ç›®æ ¹ç›®å½•
    cache_dir = PROJECT_ROOT / "outputs" / "cache"
    cache_dir.mkdir(exist_ok=True)
    
    hf_cache_dir = cache_dir / "huggingface"
    hf_cache_dir.mkdir(exist_ok=True)
    
    os.environ["HF_HOME"] = str(hf_cache_dir)
    os.environ["HF_DATASETS_CACHE"] = str(hf_cache_dir / "datasets")
    os.environ["TRANSFORMERS_CACHE"] = str(hf_cache_dir / "transformers")
    os.environ["HF_EVALUATE_CACHE"] = str(hf_cache_dir / "evaluate")
    
    print(f"""
{'='*60}
ç¦»çº¿è¯„æµ‹èµ„æºå‡†å¤‡å·¥å…·
{'='*60}
é¡¹ç›®ç›®å½•: {PROJECT_ROOT}
æ•°æ®ç›®å½•: {DATA_DIR}
ç¼“å­˜ç›®å½•: {hf_cache_dir}
{'='*60}
""")
    
    # 1. æ‰«ææœ¬åœ°æ•°æ®é›†
    print("æ­¥éª¤ 1: æ‰«ææœ¬åœ°æ•°æ®é›†")
    print("-" * 40)
    
    datasets = get_local_datasets()
    
    if not datasets:
        print("[è­¦å‘Š] æœªæ‰¾åˆ°ä»»ä½•æ•°æ®é›†!")
        print(f"è¯·å…ˆå°†æ•°æ®é›†æ”¾å…¥ {DATA_DIR} ç›®å½•")
        sys.exit(1)
    
    print(f"æ‰¾åˆ° {len(datasets)} ä¸ªæ•°æ®é›†/æ•°æ®é›†ç»„:")
    
    total_subtasks = 0
    for ds in datasets:
        if ds["is_group"]:
            subtask_count = len(ds["subtasks"])
            total_subtasks += subtask_count
            print(f"  ğŸ“ {ds['name']} (ç»„, {subtask_count} ä¸ªå­ä»»åŠ¡)")
        else:
            print(f"  ğŸ“„ {ds['name']}")
    
    print(f"\næ€»è®¡: {len(datasets)} ä¸ªæ•°æ®é›†ç»„, {total_subtasks} ä¸ªå­ä»»åŠ¡")
    
    # 2. éªŒè¯æ•°æ®é›†å®Œæ•´æ€§
    print(f"\næ­¥éª¤ 2: éªŒè¯æ•°æ®é›†å®Œæ•´æ€§")
    print("-" * 40)
    
    valid_count = 0
    invalid_count = 0
    
    for ds in datasets:
        if ds["is_group"]:
            # éªŒè¯å­ä»»åŠ¡
            for subtask in ds["subtasks"]:
                is_valid, msg = verify_dataset(subtask["path"])
                status = "âœ“" if is_valid else "âœ—"
                print(f"  {status} {ds['name']}/{subtask['name']}: {msg}")
                if is_valid:
                    valid_count += 1
                else:
                    invalid_count += 1
        else:
            is_valid, msg = verify_dataset(ds["path"])
            status = "âœ“" if is_valid else "âœ—"
            print(f"  {status} {ds['name']}: {msg}")
            if is_valid:
                valid_count += 1
            else:
                invalid_count += 1
    
    print(f"\néªŒè¯å®Œæˆ: {valid_count} ä¸ªæœ‰æ•ˆ, {invalid_count} ä¸ªæ— æ•ˆ")
    
    if args.verify_only:
        print("\n[ä»…éªŒè¯æ¨¡å¼] è·³è¿‡åç»­æ­¥éª¤")
        sys.exit(0 if invalid_count == 0 else 1)
    
    # 3. ä¸‹è½½è¯„æµ‹æŒ‡æ ‡ï¼ˆå¯é€‰ï¼‰
    if args.download_metrics:
        download_metrics(force=args.force)
    
    # 4. åˆ›å»ºç¦»çº¿é…ç½®
    print(f"\næ­¥éª¤ 3: åˆ›å»ºç¦»çº¿é…ç½®")
    print("-" * 40)
    create_offline_config()
    
    # 5. æ‰“å°ä½¿ç”¨è¯´æ˜
    print_offline_instructions()
    
    if invalid_count > 0:
        print(f"[è­¦å‘Š] æœ‰ {invalid_count} ä¸ªæ•°æ®é›†éªŒè¯å¤±è´¥ï¼Œå¯èƒ½éœ€è¦é‡æ–°ä¸‹è½½")
        sys.exit(1)


if __name__ == "__main__":
    main()
