#!/usr/bin/env python3
"""
ä¸‹è½½å¸¸ç”¨çš„ benchmarks æ•°æ®é›†åˆ°æœ¬åœ° data/ ç›®å½•
æ”¯æŒç¦»çº¿è¯„æµ‹ä½¿ç”¨
"""

import os
import sys
from pathlib import Path
from datasets import load_dataset
from tqdm import tqdm

# é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = Path(__file__).parent.parent
DATA_DIR = PROJECT_ROOT / "data"

# å¸¸ç”¨ benchmarks æ•°æ®é›†é…ç½®
# æ ¼å¼: (dataset_path, dataset_name, description)
COMMON_DATASETS = [
    # æ•°å­¦å’Œæ¨ç†
    ("gsm8k", "main", "GSM8K - æ•°å­¦é—®é¢˜æ±‚è§£"),
    # MATH æ•°æ®é›†æœ‰å¤šä¸ªå­ä»»åŠ¡ï¼Œéœ€è¦åˆ†åˆ«ä¸‹è½½
    ("EleutherAI/hendrycks_math", "algebra", "MATH Algebra - ä»£æ•°"),
    ("EleutherAI/hendrycks_math", "counting_and_probability", "MATH Counting & Probability - è®¡æ•°ä¸æ¦‚ç‡"),
    ("EleutherAI/hendrycks_math", "geometry", "MATH Geometry - å‡ ä½•"),
    ("EleutherAI/hendrycks_math", "intermediate_algebra", "MATH Intermediate Algebra - ä¸­çº§ä»£æ•°"),
    ("EleutherAI/hendrycks_math", "number_theory", "MATH Number Theory - æ•°è®º"),
    ("EleutherAI/hendrycks_math", "prealgebra", "MATH Prealgebra - é¢„ä»£æ•°"),
    ("EleutherAI/hendrycks_math", "precalculus", "MATH Precalculus - å¾®ç§¯åˆ†é¢„å¤‡"),
    
    # å¸¸è¯†æ¨ç†
    ("Rowan/hellaswag", None, "HellaSwag - å¸¸è¯†æ¨ç†"),
    ("winogrande", "winogrande_xl", "WinoGrande - å¸¸è¯†æ¨ç†"),
    ("baber/piqa", None, "PIQA - ç‰©ç†å¸¸è¯†é—®ç­”"),
    
    # é—®ç­”
    ("allenai/ai2_arc", "ARC-Easy", "ARC Easy - ç§‘å­¦é—®ç­”"),
    ("allenai/ai2_arc", "ARC-Challenge", "ARC Challenge - ç§‘å­¦é—®ç­”"),
    ("super_glue", "boolq", "BoolQ - å¸ƒå°”é—®ç­”"),
    ("trivia_qa", "rc.nocontext", "TriviaQA - é—®ç­”"),
    
    # è¯­è¨€å»ºæ¨¡
    ("EleutherAI/lambada_openai", "default", "LAMBADA OpenAI - è¯­è¨€å»ºæ¨¡"),
    ("wikitext", "wikitext-2-raw-v1", "WikiText-2 - è¯­è¨€å»ºæ¨¡"),
    
    # å¤šä»»åŠ¡ç†è§£
    ("cais/mmlu", "all", "MMLU - å¤šä»»åŠ¡è¯­è¨€ç†è§£"),
    
    # çœŸå®æ€§è¯„ä¼°
    ("truthful_qa", "generation", "TruthfulQA - çœŸå®æ€§è¯„ä¼°"),
    
    # å…¶ä»–
    ("sciq", None, "SciQ - ç§‘å­¦é—®ç­”"),
    ("openbookqa", "main", "OpenBookQA - å¼€æ”¾ä¹¦é—®ç­”"),
    # æ³¨æ„: social_i_qa ä½¿ç”¨æ—§è„šæœ¬æ ¼å¼ï¼Œå¯èƒ½æ— æ³•ç›´æ¥ä¸‹è½½
    # ("social_i_qa", None, "Social IQA - ç¤¾äº¤æ¨ç†"),
]

# å¯é€‰çš„å¤§å‹æ•°æ®é›†ï¼ˆä¸‹è½½æ—¶é—´è¾ƒé•¿ï¼‰
LARGE_DATASETS = [
    ("pile", None, "The Pile - å¤§è§„æ¨¡è¯­è¨€å»ºæ¨¡æ•°æ®é›†"),
]

def download_dataset(dataset_path: str, dataset_name: str | None, save_dir: Path, skip_existing: bool = True) -> bool:
    """ä¸‹è½½å•ä¸ªæ•°æ®é›†"""
    try:
        # åˆ›å»ºä¿å­˜ç›®å½•å
        save_path = save_dir / dataset_path.replace("/", "_")
        if dataset_name:
            save_path = save_path / dataset_name
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        if skip_existing and save_path.exists():
            print(f"â­  è·³è¿‡ (å·²å­˜åœ¨): {dataset_path}" + (f" ({dataset_name})" if dataset_name else ""))
            return True
        
        print(f"\næ­£åœ¨ä¸‹è½½: {dataset_path}" + (f" ({dataset_name})" if dataset_name else ""))
        
        # åŠ è½½æ•°æ®é›†
        # æ³¨æ„: æ–°ç‰ˆæœ¬çš„ datasets åº“ä¸å†æ”¯æŒ trust_remote_code
        try:
            if dataset_name:
                dataset = load_dataset(dataset_path, dataset_name)
            else:
                dataset = load_dataset(dataset_path)
        except Exception as e:
            error_msg = str(e)
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æŒ‡å®šé…ç½®åç§°
            if "Config name is missing" in error_msg or "Please pick one among" in error_msg:
                print(f"  âš ï¸  æ­¤æ•°æ®é›†éœ€è¦æŒ‡å®šé…ç½®åç§°")
                print(f"  ğŸ’¡ æç¤º: è¯·æ£€æŸ¥æ•°æ®é›†æ–‡æ¡£ï¼ŒæŒ‡å®šæ­£ç¡®çš„ dataset_name")
                raise ValueError(f"æ•°æ®é›† {dataset_path} éœ€è¦æŒ‡å®šé…ç½®åç§°") from e
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ—§è„šæœ¬æ ¼å¼
            elif "Dataset scripts are no longer supported" in error_msg:
                print(f"  âš ï¸  æ­¤æ•°æ®é›†ä½¿ç”¨æ—§è„šæœ¬æ ¼å¼ï¼Œæ— æ³•ç›´æ¥ä¸‹è½½")
                print(f"  ğŸ’¡ æç¤º: è¯¥æ•°æ®é›†å¯èƒ½å·²è¿ç§»æˆ–éœ€è¦æ‰‹åŠ¨ä¸‹è½½")
                raise RuntimeError(f"æ•°æ®é›† {dataset_path} ä½¿ç”¨æ—§è„šæœ¬æ ¼å¼ï¼Œæ— æ³•ä¸‹è½½") from e
            else:
                raise
        
        # åˆ›å»ºä¿å­˜ç›®å½•
        save_path.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜æ•°æ®é›†
        dataset.save_to_disk(str(save_path))
        
        # æ‰“å°æ•°æ®é›†ä¿¡æ¯
        print(f"âœ“ å·²ä¿å­˜åˆ°: {save_path}")
        for split_name, split_data in dataset.items():
            print(f"  - {split_name}: {len(split_data)} ä¸ªæ ·æœ¬")
        
        return True
        
    except Exception as e:
        print(f"âœ— ä¸‹è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="ä¸‹è½½ benchmarks æ•°æ®é›†åˆ°æœ¬åœ°")
    parser.add_argument(
        "--include-large",
        action="store_true",
        help="åŒ…å«å¤§å‹æ•°æ®é›†ï¼ˆä¸‹è½½æ—¶é—´è¾ƒé•¿ï¼‰"
    )
    parser.add_argument(
        "--force",
        action="store_true",
        help="å¼ºåˆ¶é‡æ–°ä¸‹è½½å·²å­˜åœ¨çš„æ•°æ®é›†"
    )
    args = parser.parse_args()
    
    # åˆ›å»º data ç›®å½•
    DATA_DIR.mkdir(exist_ok=True)
    print(f"æ•°æ®é›†å°†ä¿å­˜åˆ°: {DATA_DIR.absolute()}")
    
    # é€‰æ‹©è¦ä¸‹è½½çš„æ•°æ®é›†
    datasets_to_download = COMMON_DATASETS.copy()
    if args.include_large:
        datasets_to_download.extend(LARGE_DATASETS)
        print("\nâš ï¸  å·²åŒ…å«å¤§å‹æ•°æ®é›†ï¼Œä¸‹è½½å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´")
    
    # ç»Ÿè®¡
    success_count = 0
    fail_count = 0
    skip_count = 0
    
    # ä¸‹è½½æ‰€æœ‰æ•°æ®é›†
    for dataset_path, dataset_name, description in datasets_to_download:
        print(f"\n{'='*60}")
        print(f"æ•°æ®é›†: {description}")
        print(f"è·¯å¾„: {dataset_path}" + (f" | é…ç½®: {dataset_name}" if dataset_name else ""))
        
        result = download_dataset(
            dataset_path, 
            dataset_name, 
            DATA_DIR, 
            skip_existing=not args.force
        )
        
        if result:
            # æ£€æŸ¥æ˜¯å¦æ˜¯è·³è¿‡
            save_path = DATA_DIR / dataset_path.replace("/", "_")
            if dataset_name:
                save_path = save_path / dataset_name
            if save_path.exists() and not args.force:
                # å¯èƒ½æ˜¯è·³è¿‡çš„
                skip_count += 1
            success_count += 1
        else:
            fail_count += 1
    
    # æ‰“å°æ€»ç»“
    print(f"\n{'='*60}")
    print(f"ä¸‹è½½å®Œæˆ!")
    print(f"æˆåŠŸ: {success_count} ä¸ª")
    if skip_count > 0:
        print(f"è·³è¿‡ (å·²å­˜åœ¨): {skip_count} ä¸ª")
    print(f"å¤±è´¥: {fail_count} ä¸ª")
    print(f"\næ•°æ®é›†ä¿å­˜åœ¨: {DATA_DIR.absolute()}")
    print("\nä½¿ç”¨æ–¹æ³•:")
    print("æ•°æ®é›†ä¼šè‡ªåŠ¨ç¼“å­˜åœ¨ HuggingFace ç¼“å­˜ç›®å½•ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨ã€‚")
    print("å¦‚éœ€æŒ‡å®šæœ¬åœ°è·¯å¾„ï¼Œåœ¨ä»»åŠ¡ YAML é…ç½®ä¸­ä½¿ç”¨:")
    print("  dataset_kwargs:")
    print("    data_dir: ./data/dataset_name/")

if __name__ == "__main__":
    main()

