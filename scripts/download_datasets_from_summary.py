#!/usr/bin/env python3
"""
æ ¹æ® EVALUATION_DATASETS_SUMMARY.md æ–‡æ¡£ä¸­æåˆ°çš„æ•°æ®é›†ï¼Œ
ä»å¯¹åº”çš„ lm_eval/tasks é…ç½®æ–‡ä»¶ä¸­æå– dataset_path å’Œ dataset_nameï¼Œ
å¹¶ä¸‹è½½åˆ° /data ç›®å½•
"""

import sys
from pathlib import Path
from typing import Dict, List, Set, Optional
from datasets import load_dataset

# é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = Path(__file__).parent.parent
DATA_DIR = PROJECT_ROOT / "data"
TASKS_DIR = PROJECT_ROOT / "lm_eval" / "tasks"

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(PROJECT_ROOT))


# ä»»åŠ¡åç§°æ˜ å°„ï¼ˆæ–‡æ¡£ä¸­çš„åç§° -> å®é™…ä»»åŠ¡åç§°ï¼‰
TASK_NAME_MAPPING = {
    "BBH": "bbh",
    "BigBenchHard": "bbh",
    "MMLU-Pro": "mmlu_pro",
    "Math-hard": "leaderboard_math_hard",
    "IFEval": "ifeval",
    "Musr": "leaderboard_musr",
    "GSM8K": "gsm8k",
    "MATH": "hendrycks_math",
    "Hendrycks Math": "hendrycks_math",
    "HellaSwag": "hellaswag",
    "WinoGrande": "winogrande",
    "PIQA": "piqa",
    "ARC": "arc",
    "AI2 ARC": "arc",
    "MMLU": "mmlu",
    "TriviaQA": "triviaqa",
    "BoolQ": "super_glue",
    "OpenBookQA": "openbookqa",
    "LAMBADA": "lambada",
    "WikiText": "wikitext",
    "TruthfulQA": "truthfulqa",
    "HumanEval": "humaneval",
    "SuperGLUE": "super_glue",
    "GLUE": "glue",
    "SciQ": "sciq",
    "C-Eval": "ceval",
    "AGIEval": "agieval",
    "LogiQA": "logiqa",
    "LogiQA 2.0": "logiqa2",
    "XQuAD": "xquad",
    "LongBench": "longbench",
}


def get_task_names_from_mapping() -> List[str]:
    """ä» TASK_NAME_MAPPING ä¸­è·å–æ‰€æœ‰ä»»åŠ¡åç§°ï¼ˆæŒ‰é•¿åº¦é™åºæ’åºï¼‰
    
    è¿”å›åˆ—è¡¨è€Œä¸æ˜¯é›†åˆï¼Œç¡®ä¿æ›´é•¿çš„ä»»åŠ¡åç§°ä¼˜å…ˆåŒ¹é…
    ä¾‹å¦‚ï¼šmmlu_pro åº”è¯¥åœ¨ mmlu ä¹‹å‰åŒ¹é…ï¼Œé¿å… mmlu_prox è¢« mmlu è¯¯åŒ¹é…
    """
    # è¿”å›æ˜ å°„ä¸­çš„æ‰€æœ‰å€¼ï¼ˆå»é‡ï¼‰ï¼ŒæŒ‰é•¿åº¦é™åºæ’åº
    task_names = list(set(TASK_NAME_MAPPING.values()))
    # æŒ‰é•¿åº¦é™åºæ’åºï¼Œç¡®ä¿æ›´é•¿çš„ä»»åŠ¡åç§°ä¼˜å…ˆåŒ¹é…
    task_names.sort(key=len, reverse=True)
    return task_names


def find_yaml_files_for_tasks(task_names: List[str]) -> List[Dict]:
    """ç›´æ¥æŸ¥æ‰¾ä»»åŠ¡ç›®å½•ä¸‹çš„ YAML æ–‡ä»¶å¹¶æå–é…ç½®"""
    try:
        from lm_eval import utils
        
        configs = []
        found_tasks = set()
        
        # éå†ä»»åŠ¡ç›®å½•
        for task_dir in TASKS_DIR.iterdir():
            if not task_dir.is_dir() or task_dir.name.startswith("_"):
                continue
            
            task_dir_name = task_dir.name
            
            # ä¸¥æ ¼åŒ¹é…ï¼šåªåŒ¹é…å®Œå…¨ç›¸ç­‰æˆ–ç²¾ç¡®å‰ç¼€åŒ¹é…
            # åªåŒ¹é…ï¼š1) å®Œå…¨ç›¸ç­‰ 2) ä»¥ task_name + "_" å¼€å¤´ï¼ˆç¡®ä¿æ˜¯å­ä»»åŠ¡ï¼Œä¸æ˜¯å˜ä½“ï¼‰
            # æ’é™¤æ‰€æœ‰å·²çŸ¥çš„å˜ä½“ç›®å½•
            excluded_dirs = {
                "mmlu": ["mmlu_prox", "mmlu-redux", "mmlu-redux-spanish", "mmlu-pro-plus", 
                        "mmlusr", "afrimmlu", "arabicmmlu", "darijammlu", "egymmlu", 
                        "global_mmlu", "kmmlu", "turkishmmlu", "cmmlu", "tmmluplus"],
                "mmlu_pro": ["mmlu_prox"],
                "glue": ["basqueglue", "code_x_glue"],
                "hellaswag": ["darijahellaswag", "egyhellaswag"],
                "winogrande": ["icelandic_winogrande"],
                "mgsm": ["afrimgsm"],
                "piqa": ["global_piqa"],
                "truthfulqa": ["truthfulqa-multi"],
                "longbench": ["longbench2"],
                "logiqa": [],  # logiqa2 æ˜¯ç‹¬ç«‹çš„ï¼Œä¸ä¼šè¢« logiqa åŒ¹é…åˆ°ï¼ˆå› ä¸º logiqa2 ä¸ä»¥ logiqa_ å¼€å¤´ï¼‰
                "arc": ["arc_mt"],  # arc_mt æ˜¯å¤šè¯­è¨€ç‰ˆæœ¬ï¼Œä¸æ˜¯æ ¸å¿ƒ ARC
                "gsm8k": ["gsm8k_platinum"],  # gsm8k_platinum æ˜¯å˜ä½“
                "humaneval": ["humaneval_infilling"],  # humaneval_infilling æ˜¯å˜ä½“
                "lambada": ["lambada_cloze", "lambada_multilingual", "lambada_multilingual_stablelm"],  # åªä¿ç•™æ ¸å¿ƒ lambada
            }
            
            matched = False
            for task_name in task_names:
                # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æ˜¯è¢«æ’é™¤çš„ç›®å½•
                if task_name in excluded_dirs:
                    if task_dir_name in excluded_dirs[task_name]:
                        continue
                
                # ä¸¥æ ¼åŒ¹é…ï¼šå®Œå…¨ç›¸ç­‰æˆ–ç²¾ç¡®å‰ç¼€åŒ¹é…ï¼ˆtask_name + "_"ï¼‰
                if task_dir_name == task_name:
                    matched = True
                    break
                elif task_dir_name.startswith(task_name + "_"):
                    # ç¡®ä¿ä¸æ˜¯è¢«æ’é™¤çš„å˜ä½“
                    # ä¾‹å¦‚ï¼šmmlu_prox ä¸åº”è¯¥è¢« mmlu åŒ¹é…ï¼ˆè™½ç„¶å®ƒä¸ä»¥ mmlu_ å¼€å¤´ï¼Œä½†ä¸ºäº†å®‰å…¨ï¼‰
                    matched = True
                    break
            
            if not matched:
                continue
            
            # æŸ¥æ‰¾è¯¥ç›®å½•ä¸‹çš„æ‰€æœ‰ YAML æ–‡ä»¶
            yaml_files = list(task_dir.glob("*.yaml"))
            # ä¹ŸæŸ¥æ‰¾å­ç›®å½•ä¸­çš„ YAML æ–‡ä»¶
            for subdir in task_dir.iterdir():
                if subdir.is_dir() and not subdir.name.startswith("_"):
                    yaml_files.extend(subdir.glob("*.yaml"))
            
            for yaml_file in yaml_files:
                # è·³è¿‡ä»¥ _ å¼€å¤´çš„æ–‡ä»¶ï¼ˆé€šå¸¸æ˜¯æ¨¡æ¿æ–‡ä»¶ï¼‰
                if yaml_file.name.startswith("_"):
                    continue
                
                try:
                    config = utils.load_yaml_config(str(yaml_file), mode="simple")
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ç»„é…ç½®æ–‡ä»¶ï¼ˆæœ‰ group å­—æ®µï¼‰
                    if "group" in config:
                        # è¿™æ˜¯ç»„é…ç½®æ–‡ä»¶ï¼Œè·³è¿‡ï¼ˆç»„å†…çš„å­ä»»åŠ¡ä¼šå•ç‹¬å¤„ç†ï¼‰
                        continue
                    
                    task_name_in_config = config.get("task")
                    dataset_path = config.get("dataset_path")
                    
                    if not dataset_path:
                        continue
                    
                    # è®¡ç®—ç›¸å¯¹äº TASKS_DIR çš„è·¯å¾„ï¼Œç”¨äºç¡®å®šä¿å­˜ç›®å½•ç»“æ„
                    relative_path = yaml_file.relative_to(TASKS_DIR)
                    # è·å–ä»»åŠ¡ç›®å½•åï¼ˆç¬¬ä¸€çº§ç›®å½•ï¼‰
                    task_folder = relative_path.parts[0]
                    
                    # ä¸¥æ ¼åŒ¹é…ä»»åŠ¡åç§°ï¼šåªåŒ¹é…å®Œå…¨ç›¸ç­‰æˆ–ç²¾ç¡®å‰ç¼€åŒ¹é…
                    # æ”¯æŒä¸¤ç§å‰ç¼€æ ¼å¼ï¼štask_name + "_" æˆ– task_name + "-"ï¼ˆå¦‚ ceval-valid_*ï¼‰
                    # æ’é™¤æ‰€æœ‰å·²çŸ¥çš„å˜ä½“ä»»åŠ¡
                    excluded_variants = {
                        "mmlu": ["mmlu_prox", "mmlu_prox_lite", "mmlu_prox_", "mmlu_prox_lite_",
                                "mmlu_redux", "mmlu_redux_", "mmlu_llama", "mmlu_cot_llama"],
                        "mmlu_pro": ["mmlu_prox", "mmlu_prox_lite", "mmlu_prox_", "mmlu_prox_lite_"],
                        "arc": ["arc_mt", "arc_challenge_mt"],  # arc_mt æ˜¯å¤šè¯­è¨€ç‰ˆæœ¬
                        "gsm8k": ["gsm8k_platinum"],  # gsm8k_platinum æ˜¯å˜ä½“
                        "humaneval": ["humaneval_multi_line_infilling", "humaneval_single_line_infilling", 
                                     "humaneval_random_span_infilling", "humaneval_infilling"],  # infilling å˜ä½“
                        "lambada": ["lambada_openai_cloze", "lambada_multilingual", "lambada_multilingual_stablelm"],  # åªä¿ç•™æ ¸å¿ƒ lambada
                    }
                    
                    if task_name_in_config:
                        for task_name in task_names:
                            # æ£€æŸ¥æ˜¯å¦æ˜¯è¢«æ’é™¤çš„å˜ä½“
                            excluded = False
                            if task_name in excluded_variants:
                                for variant in excluded_variants[task_name]:
                                    if task_name_in_config.startswith(variant):
                                        excluded = True
                                        break
                            
                            if excluded:
                                continue
                            
                            # ä¸¥æ ¼åŒ¹é…ï¼šå®Œå…¨ç›¸ç­‰æˆ–ç²¾ç¡®å‰ç¼€åŒ¹é…
                            # æ”¯æŒ task_name + "_" æˆ– task_name + "-" å‰ç¼€
                            if (task_name_in_config == task_name or 
                                task_name_in_config.startswith(task_name + "_") or
                                task_name_in_config.startswith(task_name + "-")):
                                key = (task_folder, dataset_path, config.get("dataset_name"))
                                if key not in found_tasks:
                                    configs.append({
                                        "task_name": task_name_in_config,
                                        "dataset_path": dataset_path,
                                        "dataset_name": config.get("dataset_name"),
                                        "task_folder": task_folder,  # ä¿å­˜ä»»åŠ¡ç›®å½•å
                                        "yaml_path": str(yaml_file)
                                    })
                                    found_tasks.add(key)
                                    break
                    else:
                        # æ²¡æœ‰ä»»åŠ¡åç§°ï¼Œä½†ç›®å½•ååŒ¹é…ï¼Œä¹Ÿæ·»åŠ 
                        key = (task_folder, dataset_path, config.get("dataset_name"))
                        if key not in found_tasks:
                            configs.append({
                                "task_name": task_dir_name,
                                "dataset_path": dataset_path,
                                "dataset_name": config.get("dataset_name"),
                                "task_folder": task_folder,  # ä¿å­˜ä»»åŠ¡ç›®å½•å
                                "yaml_path": str(yaml_file)
                            })
                            found_tasks.add(key)
                            
                except Exception as e:
                    # å¿½ç•¥åŠ è½½å¤±è´¥çš„æ–‡ä»¶
                    pass
        
        return configs
        
    except Exception as e:
        print(f"æŸ¥æ‰¾ YAML æ–‡ä»¶å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return []


def deduplicate_configs(configs: List[Dict]) -> List[Dict]:
    """å»é‡é…ç½®ï¼ˆç›¸åŒçš„ task_folder, dataset_path å’Œ dataset_name åªä¿ç•™ä¸€ä¸ªï¼‰"""
    seen = set()
    unique_configs = []
    
    for config in configs:
        # ä½¿ç”¨ task_folder, dataset_path å’Œ dataset_name ä½œä¸ºå”¯ä¸€é”®
        key = (config.get("task_folder"), config["dataset_path"], config.get("dataset_name"))
        if key not in seen:
            seen.add(key)
            unique_configs.append(config)
    
    return unique_configs


def download_dataset(dataset_path: str, dataset_name: Optional[str], task_folder: str, save_dir: Path, skip_existing: bool = True) -> bool:
    """ä¸‹è½½å•ä¸ªæ•°æ®é›†
    
    Args:
        dataset_path: HuggingFace æ•°æ®é›†è·¯å¾„ï¼ˆå¦‚ allenai/ai2_arcï¼‰
        dataset_name: æ•°æ®é›†é…ç½®åç§°ï¼ˆå¦‚ ARC-Easyï¼‰
        task_folder: ä»»åŠ¡ç›®å½•åï¼ˆå¦‚ arcï¼‰ï¼Œç”¨äºä¿æŒç›®å½•ç»“æ„
        save_dir: ä¿å­˜æ ¹ç›®å½•
        skip_existing: æ˜¯å¦è·³è¿‡å·²å­˜åœ¨çš„æ•°æ®é›†
    """
    try:
        # ä½¿ç”¨ä»»åŠ¡ç›®å½•ç»“æ„ï¼Œä¿æŒä¸ lm_eval/tasks ç›¸åŒçš„æ–‡ä»¶å¤¹ç»“æ„
        # ä¾‹å¦‚ï¼šlm_eval/tasks/arc/ -> data/arc/
        save_path = save_dir / task_folder
        if dataset_name:
            save_path = save_path / dataset_name
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        if skip_existing and save_path.exists():
            print(f"â­  è·³è¿‡ (å·²å­˜åœ¨): {dataset_path}" + (f" ({dataset_name})" if dataset_name else ""))
            return True
        
        print(f"\næ­£åœ¨ä¸‹è½½: {dataset_path}" + (f" ({dataset_name})" if dataset_name else ""))
        
        # åŠ è½½æ•°æ®é›†
        try:
            # æ³¨æ„: æ–°ç‰ˆæœ¬çš„ datasets åº“ä¸å†æ”¯æŒ trust_remote_code
            if dataset_name:
                dataset = load_dataset(dataset_path, dataset_name)
            else:
                dataset = load_dataset(dataset_path)
        except Exception as e:
            error_msg = str(e)
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æŒ‡å®šé…ç½®åç§°
            if "Config name is missing" in error_msg or "Please pick one among" in error_msg:
                print(f"  âš ï¸  æ­¤æ•°æ®é›†éœ€è¦æŒ‡å®šé…ç½®åç§°")
                print(f"  ğŸ’¡ æç¤º: è¯·æ£€æŸ¥æ•°æ®é›†æ–‡æ¡£ï¼ŒæŒ‡å®šæ­£ç¡®çš„ dataset_name")
                return False
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ—§è„šæœ¬æ ¼å¼
            elif "Dataset scripts are no longer supported" in error_msg or "isn't based on a loading script" in error_msg:
                print(f"  âš ï¸  æ­¤æ•°æ®é›†ä½¿ç”¨æ—§è„šæœ¬æ ¼å¼ï¼Œæ— æ³•ç›´æ¥ä¸‹è½½")
                print(f"  ğŸ’¡ æç¤º: è¯¥æ•°æ®é›†å¯èƒ½å·²è¿ç§»æˆ–éœ€è¦æ‰‹åŠ¨ä¸‹è½½")
                return False
            # æ£€æŸ¥ä»£ç†ç›¸å…³é”™è¯¯
            elif "SOCKS proxy" in error_msg or "socksio" in error_msg:
                print(f"  âš ï¸  ä»£ç†é…ç½®é—®é¢˜: {error_msg}")
                print(f"  ğŸ’¡ æç¤º: å¦‚æœéœ€è¦ä½¿ç”¨ SOCKS ä»£ç†ï¼Œè¯·å®‰è£…: pip install httpx[socks]")
                print(f"  ğŸ’¡ æˆ–è€…ç¦ç”¨ä»£ç†ç¯å¢ƒå˜é‡åé‡è¯•")
                return False
            else:
                raise
        
        # åªä¿ç•™ test å’Œ validation splitsï¼Œç§»é™¤ train
        splits_to_keep = []
        splits_to_remove = []
        for split_name in dataset.keys():
            split_lower = split_name.lower()
            # ä¿ç•™ test å’Œ validationï¼ˆåŒ…æ‹¬å„ç§å˜ä½“ï¼‰
            if any(x in split_lower for x in ['test', 'val', 'dev', 'validation']):
                splits_to_keep.append(split_name)
            else:
                splits_to_remove.append(split_name)
        
        # åˆ›å»ºåªåŒ…å« test å’Œ validation çš„æ•°æ®é›†
        filtered_dataset = {}
        for split_name in splits_to_keep:
            filtered_dataset[split_name] = dataset[split_name]
        
        if not filtered_dataset:
            print(f"  âš ï¸  æ•°æ®é›†æ²¡æœ‰ test æˆ– validation splitï¼Œè·³è¿‡")
            return False
        
        # ç¡®è®¤å¹¶åˆ›å»ºä¿å­˜ç›®å½•
        if not save_path.exists():
            print(f"  åˆ›å»ºç›®å½•: {save_path}")
            save_path.mkdir(parents=True, exist_ok=True)
        elif not save_path.is_dir():
            print(f"  âš ï¸  è·¯å¾„å·²å­˜åœ¨ä½†ä¸æ˜¯ç›®å½•: {save_path}")
            return False
        else:
            print(f"  ç›®å½•å·²å­˜åœ¨: {save_path}")
        
        # ä¿å­˜è¿‡æ»¤åçš„æ•°æ®é›†
        from datasets import DatasetDict
        filtered_dataset_dict = DatasetDict(filtered_dataset)
        filtered_dataset_dict.save_to_disk(str(save_path))
        
        # æ‰“å°æ•°æ®é›†ä¿¡æ¯
        print(f"âœ“ å·²ä¿å­˜åˆ°: {save_path}")
        for split_name, split_data in filtered_dataset.items():
            print(f"  - {split_name}: {len(split_data)} ä¸ªæ ·æœ¬")
        if splits_to_remove:
            print(f"  â­  å·²è·³è¿‡ train splits: {', '.join(splits_to_remove)}")
        
        return True
        
    except Exception as e:
        print(f"âœ— ä¸‹è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="æ ¹æ®æ±‡æ€»æ–‡æ¡£ä¸‹è½½è¯„æµ‹æ•°æ®é›†")
    parser.add_argument(
        "--force",
        action="store_true",
        help="å¼ºåˆ¶é‡æ–°ä¸‹è½½å·²å­˜åœ¨çš„æ•°æ®é›†"
    )
    parser.add_argument(
        "--category",
        type=str,
        choices=["all", "general", "chinese"],
        default="all",
        help="é€‰æ‹©è¦ä¸‹è½½çš„æ•°æ®é›†ç±»åˆ«: all(å…¨éƒ¨), general(é€šç”¨), chinese(ä¸­æ–‡)"
    )
    args = parser.parse_args()
    
    # åˆ›å»º data ç›®å½•
    DATA_DIR.mkdir(exist_ok=True)
    print(f"æ•°æ®é›†å°†ä¿å­˜åˆ°: {DATA_DIR.absolute()}")
    print("=" * 60)
    
    # ä» TASK_NAME_MAPPING è·å–ä»»åŠ¡åç§°
    print("\nä» TASK_NAME_MAPPING è·å–ä»»åŠ¡åç§°...")
    task_names = get_task_names_from_mapping()
    print(f"æ‰¾åˆ° {len(task_names)} ä¸ªä»»åŠ¡åç§°")
    
    # æ ¹æ®ç±»åˆ«è¿‡æ»¤
    if args.category == "general":
        # é€šç”¨æ•°æ®é›†ï¼ˆæ’é™¤ä¸­æ–‡ç‰¹å®šçš„ï¼‰
        chinese_tasks = {"ceval", "cmmlu", "tmlu", "tmmluplus", "aclue", "agieval", 
                        "logiqa", "logiqa2", "zhoblimp", "xquad", "mlqa", 
                        "mgsm", "longbench"}
        task_names = [t for t in task_names if t not in chinese_tasks]
    elif args.category == "chinese":
        # åªä¿ç•™ä¸­æ–‡æ•°æ®é›†
        chinese_tasks = {"ceval", "cmmlu", "tmlu", "tmmluplus", "aclue", "agieval", 
                        "logiqa", "logiqa2", "zhoblimp"}
        task_names = [t for t in task_names if t in chinese_tasks or any(ct in t for ct in chinese_tasks)]
    
    # é‡æ–°æ’åºï¼ˆæŒ‰é•¿åº¦é™åºï¼‰
    task_names.sort(key=len, reverse=True)
    
    print(f"è¿‡æ»¤åå‰©ä½™ {len(task_names)} ä¸ªä»»åŠ¡")
    print(f"ä»»åŠ¡åˆ—è¡¨: {', '.join(sorted(task_names))}")
    
    # æŸ¥æ‰¾ YAML æ–‡ä»¶å¹¶æå–é…ç½®
    print("\næŸ¥æ‰¾ä»»åŠ¡é…ç½®æ–‡ä»¶...")
    configs = find_yaml_files_for_tasks(task_names)
    print(f"æ‰¾åˆ° {len(configs)} ä¸ªä»»åŠ¡é…ç½®")
    
    # å»é‡
    configs = deduplicate_configs(configs)
    print(f"å»é‡åå‰©ä½™ {len(configs)} ä¸ªå”¯ä¸€æ•°æ®é›†é…ç½®")
    
    # ç»Ÿè®¡
    success_count = 0
    fail_count = 0
    skip_count = 0
    
    # ä¸‹è½½æ‰€æœ‰æ•°æ®é›†
    print("\n" + "=" * 60)
    print("å¼€å§‹ä¸‹è½½æ•°æ®é›†...")
    print("=" * 60)
    
    for config in configs:
        dataset_path = config["dataset_path"]
        dataset_name = config.get("dataset_name")
        task_name = config.get("task_name", "unknown")
        task_folder = config.get("task_folder", "unknown")
        
        print(f"\nä»»åŠ¡: {task_name}")
        print(f"æ•°æ®é›†: {dataset_path}" + (f" | é…ç½®: {dataset_name}" if dataset_name else ""))
        print(f"ä¿å­˜åˆ°: data/{task_folder}/" + (f"{dataset_name}/" if dataset_name else ""))
        
        result = download_dataset(
            dataset_path, 
            dataset_name,
            task_folder,
            DATA_DIR, 
            skip_existing=not args.force
        )
        
        if result:
            # æ£€æŸ¥æ˜¯å¦æ˜¯è·³è¿‡
            save_path = DATA_DIR / task_folder
            if dataset_name:
                save_path = save_path / dataset_name
            if save_path.exists() and not args.force:
                skip_count += 1
            success_count += 1
        else:
            fail_count += 1
    
    # æ‰“å°æ€»ç»“
    print(f"\n{'='*60}")
    print(f"ä¸‹è½½å®Œæˆ!")
    print(f"æˆåŠŸ: {success_count} ä¸ª")
    if skip_count > 0:
        print(f"è·³è¿‡ (å·²å­˜åœ¨): {skip_count} ä¸ª")
    print(f"å¤±è´¥: {fail_count} ä¸ª")
    print(f"\næ•°æ®é›†ä¿å­˜åœ¨: {DATA_DIR.absolute()}")


if __name__ == "__main__":
    main()

