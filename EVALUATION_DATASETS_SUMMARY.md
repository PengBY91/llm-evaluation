# lm-evaluation-harness 评测数据集汇总

本文档汇总了 `lm-evaluation-harness` 中最常用的评测数据集，包括通用评测集和专门针对中文能力的评测集。

---

## 📊 一、通用评测数据集

### 1. Leaderboard 评测集（HuggingFace Open LLM Leaderboard）

这些是官方 leaderboard 使用的评测集：

- **BBH (BigBenchHard)**：23 个挑战性推理任务
- **MMLU-Pro**：增强版 MMLU，更难的推理题
- **GPQA**：研究生级别科学问答
- **Math-hard**：MATH 数据集中的 Level 5 题目（4-shot）
- **IFEval**：指令遵循能力评估
- **Musr**：多步软推理任务

### 2. 数学推理

- **GSM8K**：小学数学问题求解
- **MATH (Hendrycks Math)**：竞赛数学题，包含代数、几何、数论等 7 个子领域

### 3. 常识推理

- **HellaSwag**：预测故事结尾
- **WinoGrande**：常识推理
- **PIQA**：物理常识问答

### 4. 问答与阅读理解

- **ARC (AI2 ARC)**：科学问答（Easy/Challenge）
- **MMLU**：多任务语言理解（57 个学科）
- **TriviaQA**：开放域问答
- **BoolQ**：布尔问答（SuperGLUE）
- **OpenBookQA**：开放书问答

### 5. 语言建模

- **LAMBADA**：预测文本结尾
- **WikiText**：语言建模基准

### 6. 真实性评估

- **TruthfulQA**：真实性评估

### 7. 代码生成

- **HumanEval**：Python 代码生成

### 8. 其他常用

- **SuperGLUE**：语言理解基准套件
- **GLUE**：通用语言理解评估
- **SciQ**：科学问答

---

## 🇨🇳 二、专门针对中文能力的评测数据集

### 1. 综合知识评测

#### C-Eval (`ceval`)
- **规模**：52 个学科，13,948 道多选题
- **难度级别**：覆盖初中、高中、大学、专业资格等 4 个难度级别
- **学科范围**：计算机、数学、物理、化学、生物、历史、政治、法律等
- **特点**：全面的中文知识评测基准

#### CMMLU (`cmmlu`)
- **规模**：67 个主题，涵盖从小学到高级专业水平
- **特点**：评估中文语言和文化背景下的知识与推理能力
- **内容**：包含中国历史、文学、文化、法律等本土化内容
- **优势**：更贴近中文语境和文化背景

#### TMLU (`tmlu`)
- **规模**：37 个学科，2,981 道多选题
- **语言**：台湾繁体中文评测
- **内容**：涵盖社会科学、STEM、人文、台湾特定内容等
- **用途**：评估繁体中文模型能力

#### TMMLU+ (`tmmluplus`)
- **规模**：66 个学科，22,690 道多选题
- **特点**：TMLU 的扩展版本，传统中文评测套件
- **改进**：数据量是 TMLU 的 6 倍，学科分布更均衡

### 2. 古代中文理解

#### ACLUE (`aclue`)
- **规模**：15 个任务，涵盖词汇、句法、语义、推理、知识等
- **时间跨度**：从夏朝（公元前 2070 年）到明朝（1368 年）
- **任务类型**：古代文学、医学、音韵、对联、诗词等
- **特点**：专门评估模型对古代中文的理解能力

### 3. 高考与考试评测

#### AGIEval (`agieval`)
- **中文子任务**：
  - 高考语文、数学、物理、化学、生物、历史、地理等
  - 法律资格考试（JEC-QA）等
- **任务组**：`agieval_cn` 包含所有中文子任务
- **特点**：基于真实考试题目，评估模型在考试场景下的表现

### 4. 逻辑推理

#### LogiQA (`logiqa`)
- **特点**：逻辑推理阅读理解数据集
- **语言**：包含中英文版本
- **用途**：评估模型的逻辑推理能力

#### LogiQA 2.0 (`logiqa2`)
- **特点**：LogiQA 的改进版本
- **来源**：基于中国公务员考试改编
- **中文版本**：`logiqa2_zh`
- **改进**：数据量增加，文本经过专业翻译，移除了具有明显文化特征的题目

### 5. 语法评测

#### ZhoBLiMP (`zhoblimp`)
- **规模**：118 个范式，覆盖 15 种语言现象
- **特点**：中文语法最小对评测
- **用途**：评估模型对中文语法的掌握程度
- **评估方式**：通过最小对（minimal pairs）测试语法判断能力

### 6. 跨语言评测（包含中文）

#### XNLI (`xnli`)
- **任务**：跨语言自然语言推理
- **中文版本**：`xnli_zh`
- **特点**：评估跨语言理解能力

#### XQuAD (`xquad`)
- **任务**：跨语言问答数据集
- **中文版本**：`xquad_zh`
- **特点**：评估跨语言问答能力

#### MLQA (`mlqa`)
- **任务**：多语言问答评测
- **语言**：包含简体中文
- **组合**：支持跨语言问答组合（如 `mlqa_zh_zh`, `mlqa_en_zh` 等）
- **特点**：评估模型在不同语言对之间的问答能力

#### MGSM (`mgsm`)
- **任务**：多语言小学数学问题
- **中文版本**：
  - `mgsm_direct_zh`：直接问答
  - `mgsm_cot_native_zh`：中文思维链推理
- **特点**：评估中文数学推理能力

### 7. 长文本理解

#### LongBench (`longbench`)
- **任务**：长文本理解评测
- **语言**：包含中文任务
- **特点**：评估模型对长文本的理解能力

### 8. 其他中文评测

#### CNN-DailyMail (中文版) (`cnn_dailymail`)
- **任务**：摘要生成任务
- **语言**：包含中文版本
- **特点**：评估中文摘要生成能力

---

## 📋 三、数据集覆盖能力总结

### 通用能力
- ✅ 数学推理
- ✅ 常识推理
- ✅ 阅读理解
- ✅ 多任务理解
- ✅ 代码生成
- ✅ 真实性评估
- ✅ 语言建模
- ✅ 指令遵循

### 中文特定能力
- ✅ 中文综合知识（C-Eval, CMMLU）
- ✅ 古代中文理解（ACLUE）
- ✅ 中文考试能力（AGIEval）
- ✅ 中文逻辑推理（LogiQA）
- ✅ 中文语法能力（ZhoBLiMP）
- ✅ 繁体中文能力（TMLU, TMMLU+）
- ✅ 跨语言能力（XNLI, XQuAD, MLQA）
- ✅ 中文数学推理（MGSM）
- ✅ 中文长文本理解（LongBench）

---

## 🔍 四、使用建议

### 对于通用模型评测
推荐使用 Leaderboard 评测集（BBH, MMLU-Pro, GPQA, Math-hard, IFEval, Musr）进行综合评估。

### 对于中文模型评测
1. **综合知识**：C-Eval 或 CMMLU
2. **古代中文**：ACLUE
3. **考试能力**：AGIEval
4. **逻辑推理**：LogiQA 2.0
5. **语法能力**：ZhoBLiMP
6. **繁体中文**：TMLU 或 TMMLU+

### 对于多语言模型评测
可以结合通用评测集和跨语言评测集（XNLI, XQuAD, MLQA, MGSM）进行全面评估。

---

## 📚 参考资料

- [lm-evaluation-harness GitHub](https://github.com/EleutherAI/lm-evaluation-harness)
- [HuggingFace Open LLM Leaderboard](https://huggingface.co/spaces/open-llm-leaderboard)
- 各评测数据集的 README 文件位于 `lm_eval/tasks/[任务名]/README.md`

---

*最后更新：2024年*

